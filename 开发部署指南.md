# AIRP记忆系统 - Graphiti + DeepSeek V3.2 完整开发部署指南

## 目录
1. [项目概述与技术选型](#1-项目概述与技术选型)
2. [环境准备](#2-环境准备)
3. [Docker部署配置](#3-docker部署配置)
4. [Graphiti配置与兼容层设计](#4-graphiti配置与兼容层设计)
5. [API服务实现](#5-api服务实现)
6. [DeepSeek集成方案](#6-deepseek集成方案)
7. [SillyTavern连接配置](#7-sillytavern连接配置)
8. [测试与验证](#8-测试与验证)
9. [故障排除](#9-故障排除)

---

## 1. 项目概述与技术选型

### 1.1 项目目标
构建一个基于Graphiti时序知识图谱的记忆增强系统，为SillyTavern提供：
- 实时角色心理状态建模
- 世界观逻辑推演支持
- 动态记忆检索与整合
- OpenAI兼容API接口

### 1.2 核心技术栈

#### 数据库层
- **Neo4j 5.26+**: 时序知识图谱存储后端
- **理由**: Graphiti官方推荐，支持双时序模型，高性能图遍历

#### 记忆框架层
- **Graphiti-core**: Zep AI的开源时序知识图谱框架
- **版本**: 最新稳定版（通过pip安装）
- **功能**: 实体关系提取、时序跟踪、混合检索

#### LLM服务层
- **DeepSeek V3.2**: 通过OpenAI兼容API调用（LLM推理）
- **硅基流动**: Embedding和Reranker服务
- **特殊处理**: JSON Schema兼容层（下文详述）
- **备选方案**: 本地Ollama（可选）

#### API服务层
- **FastAPI**: 高性能异步Web框架
- **兼容性**: 完全兼容OpenAI Chat Completions API
- **功能**: 格式解析、记忆管理、提示词优化

---

## 2. 环境准备

### 2.1 硬件要求

#### 最低配置（开发测试）
```
CPU: 4核（支持AVX2指令集）
内存: 8GB（Neo4j至少4GB）
存储: 50GB SSD
网络: 稳定的互联网连接
```

#### 推荐配置（生产环境）
```
CPU: 8核+（支持AVX2和AVX-512）
内存: 16GB+（Neo4j 8GB，API服务4GB，系统4GB）
存储: 200GB NVMe SSD（IOPS > 3000）
网络: 1Gbps+带宽
```

### 2.2 软件要求

#### 必需软件
```
操作系统: 
- Ubuntu 20.04+ / 22.04 LTS
- macOS 11+ (Intel/Apple Silicon)
- Windows 10+ (WSL2强烈推荐)

容器化:
- Docker 20.10+
- Docker Compose v2.0+

Python环境:
- Python 3.10 或 3.11
- pip 23.0+
```

#### 网络端口要求
```
对内端口（服务间通信）:
- 7687: Neo4j Bolt协议
- 6379: Redis（可选）

对外端口（外部访问）:
- 7474: Neo4j Browser（可选择性开放）
- 8000: API服务（必须开放）
```

### 2.3 项目目录结构
```
airp-memory-system/
├── docker-compose.yaml          # Docker Compose配置
├── .env                     # 环境变量配置
├── requirements.txt           # Python依赖
├── api-service/              # API服务代码目录
│   ├── main.py             # FastAPI主入口
│   ├── config/             # 配置模块
│   │   ├── settings.py      # 应用设置
│   │   └── graphiti_config.py # Graphiti配置
│   ├── services/           # 服务层
│   │   ├── graphiti_service.py    # Graphiti操作
│   │   ├── llm_service.py        # LLM调用
│   │   └── compatibility_layer.py # JSON Schema兼容层
│   ├── models/             # 数据模型
│   ├── utils/              # 工具函数
│   │   ├── parser.py       # SillyTavern格式解析
│   │   └── dedup.py       # 去重逻辑
│   └── Dockerfile          # API服务Docker镜像
├── neo4j/                  # Neo4j数据挂载
│   ├── data/              # 数据持久化
│   ├── logs/              # 日志文件
│   └── import/            # 初始数据导入
└── logs/                   # 应用日志
    └── api/               # API服务日志
```

---

## 3. Docker部署配置

### 3.1 docker-compose.yaml配置文件

```yaml
version: '3.8'

services:
  # Neo4j图数据库服务
  neo4j:
    image: neo4j:5.26-community
    container_name: airp-neo4j
    restart: unless-stopped
    ports:
      - "7474:7474"    # HTTP Browser界面
      - "7687:7687"    # Bolt协议端口
    environment:
      # 认证配置
      - NEO4J_AUTH=${NEO4J_USER}/${NEO4J_PASSWORD}
      - NEO4J_ACCEPT_LICENSE_AGREEMENT=yes
      
      # 内存配置（根据服务器内存调整）
      - NEO4J_dbms_memory_pagecache_size=2G
      - NEO4J_dbms_memory_heap_initial__size=4G
      - NEO4J_dbms_memory_heap_max__size=4G
      
      # 插件配置
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
      
      # 日志配置
      - NEO4J_dbms_logs_debug_level=INFO
    volumes:
      # 数据持久化
      - ./neo4j/data:/data
      - ./neo4j/logs:/logs
      - ./neo4j/import:/var/lib/neo4j/import
      - ./neo4j/plugins:/plugins
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "${NEO4J_USER}", "-p", "${NEO4J_PASSWORD}", "RETURN 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - airp-network
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 6G
        reservations:
          cpus: '2'
          memory: 4G

  # Redis缓存服务（可选，推荐用于生产环境）
  redis:
    image: redis:7-alpine
    container_name: airp-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
    volumes:
      - ./redis/data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - airp-network

  # API服务
  api-service:
    build:
      context: ./api-service
      dockerfile: Dockerfile
    container_name: airp-api
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      # Neo4j连接配置
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=${NEO4J_USER}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD}
      
      # Redis配置（可选）
      - REDIS_URL=redis://redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      
      # DeepSeek API配置
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - DEEPSEEK_BASE_URL=${DEEPSEEK_BASE_URL}
      - DEEPSEEK_MODEL=${DEEPSEEK_MODEL}
      
      # Graphiti配置
      - GRAPHITI_SEMAPHORE_LIMIT=${GRAPHITI_SEMAPHORE_LIMIT}
      - GRAPHITI_TELEMETRY_ENABLED=${GRAPHITI_TELEMETRY_ENABLED}
      
      # API服务配置
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - API_WORKERS=${API_WORKERS}
      - API_LOG_LEVEL=${API_LOG_LEVEL}
      
      # 应用配置
      - APP_ENV=${APP_ENV}
      - APP_SECRET_KEY=${APP_SECRET_KEY}
    volumes:
      # 代码热重载（开发环境）
      - ./api-service:/app
      # 日志持久化
      - ./logs/api:/app/logs
    depends_on:
      neo4j:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - airp-network
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '2'
          memory: 2G

networks:
  airp-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
  redis_data:
    driver: local
```

### 3.2 .env环境变量配置文件

```env
# ============================================
# Neo4j数据库配置
# ============================================
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_secure_neo4j_password_change_this

# ============================================
# Redis缓存配置（可选）
# ============================================
REDIS_PASSWORD=your_secure_redis_password_change_this

# ============================================
# DeepSeek API配置
# ============================================
# API密钥（从DeepSeek平台获取）
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# API端点（普通端点）
DEEPSEEK_BASE_URL=https://api.deepseek.com

# 如果需要使用strict模式，使用beta端点（下文会详细说明）
# DEEPSEEK_BASE_URL=https://api.deepseek.com/beta

# 使用的模型
DEEPSEEK_MODEL=deepseek-chat

# ============================================
# Graphiti配置
# ============================================
# 并发限制（避免API 429错误）
# 如果遇到429错误，降低此值；如果速度慢，提高此值
GRAPHITI_SEMAPHORE_LIMIT=5

# 禁用遥测（可选）
GRAPHITI_TELEMETRY_ENABLED=false

# ============================================
# API服务配置
# ============================================
API_HOST=0.0.0.0
API_PORT=8000

# Worker数量（建议为CPU核心数-1）
API_WORKERS=3

# 日志级别: debug, info, warning, error, critical
API_LOG_LEVEL=info

# ============================================
# 应用配置
# ============================================
# 运行环境: development, production
APP_ENV=development

# 会话密钥（用于会话管理）
APP_SECRET_KEY=your_random_secret_key_at_least_32_characters_long

# ============================================
# 安全警告
# ============================================
# ⚠️ 重要：修改所有密码和密钥！
# ⚠️ 不要将.env文件提交到版本控制系统
# ⚠️ 生产环境请使用强密码
```

### 3.3 Dockerfile配置（API服务）

```dockerfile
# ============================================
# API服务Docker镜像构建文件
# ============================================
FROM python:3.11-slim

# 设置工作目录
WORKDIR /app

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# 复制依赖文件
COPY requirements.txt .

# 升级pip并安装依赖
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# 复制应用代码
COPY . .

# 创建非root用户（安全最佳实践）
RUN useradd -m -u 1000 appuser && \
    chown -R appuser:appuser /app

# 切换到非root用户
USER appuser

# 暴露端口
EXPOSE 8000

# 健康检查
HEALTHCHECK --interval=30s --timeout=10s --start-period=10s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# 启动命令
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4", "--log-level", "info"]
```

### 3.4 requirements.txt依赖文件

```txt
# ============================================
# Web框架
# ============================================
fastapi==0.104.1
uvicorn[standard]==0.24.0
python-multipart==0.0.6
pydantic==2.5.0
pydantic-settings==2.1.0

# ============================================
# Neo4j驱动
# ============================================
neo4j==5.19.0

# ============================================
# Graphiti核心框架（重要：正确的包名）
# ============================================
graphiti-core>=0.17.0

# ============================================
# LLM集成（OpenAI兼容）
# ============================================
openai>=1.7.0
httpx>=0.25.0

# ============================================
# 文本处理（可选，用于解析）
# ============================================
nltk>=3.8.1
# 下载nltk数据: python -m nltk.downloader punkt
# 如果需要中文支持，考虑使用jieba
# jieba>=0.42.1

# ============================================
# 缓存（可选）
# ============================================
redis>=5.0.0
hiredis>=2.2.0

# ============================================
# 配置管理
# ============================================
python-dotenv>=1.0.0

# ============================================
# 工具库
# ============================================
pytz>=2023.3
python-dateutil>=2.8.2
numpy>=1.24.0

# ============================================
# 开发工具（可选）
# ============================================
pytest>=7.4.0
pytest-asyncio>=0.21.0
black>=23.11.0
flake8>=6.1.0
```

---

## 4. Graphiti配置与兼容层设计

### 4.1 Graphiti初始化配置

#### 核心设计思路
Graphiti要求LLM支持JSON Schema的结构化输出，但DeepSeek V3.2有两种模式：
1. **Function Calling的strict模式**: 完全支持JSON Schema（需使用beta端点）
2. **JSON Object模式**: 支持基础JSON格式（标准端点，但不保证复杂Schema）

我们的策略是**优先使用strict模式**，如果失败则降级到兼容层。

#### 配置逻辑流程

```
Graphiti初始化流程：
    │
    ├─ 配置Neo4j驱动
    │   - URI: bolt://neo4j:7687
    │   - 用户/密码: 从环境变量读取
    │
    ├─ 配置LLM客户端
    │   - 检测DeepSeek端点类型
    │   - 如果是beta端点 → 使用OpenAI标准客户端（支持strict）
    │   - 如果是标准端点 → 使用兼容层客户端
    │
    ├─ 配置Embedding客户端
    │   - 使用硅基流动的Embedding API
    │   - 模型：BAAI/bge-m3（推荐）
    │   - 端点：https://api.siliconflow.cn/v1
    │
    ├─ 配置Cross-Encoder（重排序）
    │   - 使用硅基流动的Reranker API
    │   - 模型：BAAI/bge-reranker-v2-m3（推荐）
    │   - 端点：https://api.siliconflow.cn/v1
    │
    └─ 初始化Graphiti实例
        - 传入配置的驱动和客户端
        - 初始化索引和约束
```

### 4.2 JSON Schema兼容层设计

#### 方案一：使用DeepSeek Beta端点（推荐）

**优势**:
- DeepSeek完全支持JSON Schema strict模式
- 无需额外转换逻辑
- 性能和准确度最佳

**实现逻辑**:
```
使用Beta端点的条件：
    1. 修改.env中的DEEPSEEK_BASE_URL为beta端点
    2. 在Function Call工具定义中设置strict: true
    3. DeepSeek服务器端验证JSON Schema有效性
    4. 确保所有object属性都设置为required
    5. 设置additionalProperties: false

实现伪代码逻辑：

初始化DeepSeek LLM客户端（支持Strict模式）：
    - base_url: "https://api.deepseek.com/beta"
    - api_key: 从环境变量读取
    - 配置Strict模式参数
    - 在tools参数中设置strict: true

调用LLM时的工具定义示例：
    {
        "type": "function",
        "function": {
            "name": "extract_entities",
            "strict": true,  # 关键：启用strict模式
            "description": "从文本中提取实体和关系",
            "parameters": {
                "type": "object",
                "properties": {
                    "entities": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "name": {"type": "string"},
                                "type": {"type": "string"}
                            },
                            "required": ["name", "type"],
                            "additionalProperties": false
                        }
                    }
                },
                "required": ["entities"],
                "additionalProperties": false  # 必须为false
            }
        }
    }

处理流程：
    1. Graphiti生成带有JSON Schema的工具定义
    2. 调用DeepSeek API（strict模式）
    3. DeepSeek验证Schema有效性（服务器端）
    4. 返回严格符合Schema的结果
    5. Graphiti直接使用结果，无需转换
```

#### 方案二：兼容层转换（备选方案）

**使用场景**: 当Beta端点不稳定或无法访问时

**设计思路**:
```
兼容层工作流程：

第一层：提示词工程增强
    - 在系统提示词中明确JSON Schema要求
    - 提供完整的示例输出格式
    - 强调必须严格遵循Schema结构

第二层：后处理验证
    - 接收DeepSeek的JSON Object输出
    - 使用Pydantic模型验证输出结构
    - 补充缺失的必填字段（设为默认值）
    - 移除Schema中不允许的额外字段
    - 转换为Graphiti期望的格式

第三层：重试机制
    - 如果验证失败，请求重试
    - 最多重试3次
    - 提供更明确的错误提示

实现逻辑伪代码：

兼容层Client类：
    class DeepSeekCompatibilityClient:
        def __init__(self, base_client, pydantic_model):
            self.client = base_client  # 原始OpenAI客户端
            self.model = pydantic_model  # 目标Pydantic模型
        
        async def call_with_schema(self, messages, schema_definition):
            # 步骤1: 增强提示词
            enhanced_messages = self._enhance_prompt(
                messages, 
                schema_definition
            )
            
            # 步骤2: 调用DeepSeek（JSON Object模式）
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=enhanced_messages,
                response_format={"type": "json_object"}
            )
            
            # 步骤3: 解析原始JSON
            raw_json = json.loads(response.choices[0].message.content)
            
            # 步骤4: 验证和转换
            validated_data = self._validate_and_convert(
                raw_json, 
                schema_definition
            )
            
            return validated_data
        
        def _enhance_prompt(self, messages, schema):
            """在提示词中添加Schema要求"""
            schema_instruction = f"""
            【重要】输出格式要求：
            请严格按照以下JSON Schema输出结果：
            {json.dumps(schema, indent=2)}
            
            【验证规则】：
            1. 必须包含所有required字段
            2. 字段类型必须严格匹配
            3. 不要添加额外字段
            4. 嵌套对象也要遵守相同规则
            
            【示例输出】：
            {生成符合Schema的示例JSON}
            """
            
            # 将指令插入到系统消息
            enhanced = messages.copy()
            for msg in enhanced:
                if msg["role"] == "system":
                    msg["content"] += "\n\n" + schema_instruction
            
            return enhanced
        
        def _validate_and_convert(self, raw_json, schema):
            """验证并转换JSON输出"""
            try:
                # 使用Pydantic模型验证
                validated = self.model(**raw_json)
                return validated
            except ValidationError as e:
                # 处理验证错误
                corrected = self._fix_validation_errors(
                    raw_json, 
                    e.errors(), 
                    schema
                )
                return corrected
        
        def _fix_validation_errors(self, data, errors, schema):
            """修复验证错误"""
            corrected = data.copy()
            
            for error in errors:
                field = error['loc'][0]
                error_type = error['type']
                
                if error_type == 'missing':
                    # 补充缺失的必填字段
                    corrected[field] = self._get_default_value(
                        schema, 
                        field
                    )
                elif error_type == 'extra_forbidden':
                    # 移除不允许的额外字段
                    del corrected[field]
                elif error_type == 'type_error':
                    # 尝试类型转换
                    corrected[field] = self._convert_type(
                        corrected[field], 
                        schema['properties'][field]
                    )
            
            return corrected
```

### 4.3 Graphiti与硅基流动API的完整配置

#### 核心配置原则

根据Graphiti官方文档，对于OpenAI兼容服务：
- **使用OpenAIGenericClient**: 这是Graphiti提供的专用客户端类
- **自动Schema注入**: OpenAIGenericClient会确保正确的JSON Schema注入
- **避免小模型**: 使用更大的模型以准确提取数据和输出正确的JSON结构

#### 完整的配置实现

```
Graphiti配置实现架构：

第一步：创建LLM客户端（DeepSeek）

DeepSeekLLMClientFactory类：
    职责：根据配置创建DeepSeek LLM客户端
    
    工厂方法：
        create_deepseek_client(config) → OpenAI兼容客户端
        
    决策逻辑：
        IF config.base_url == "https://api.deepseek.com/beta":
            # Beta端点：支持Strict模式
            client = OpenAI(
                api_key=config.api_key,
                base_url=config.base_url
            )
            # 在Graphiti调用时会自动使用strict模式
            RETURN client
        ELSE:
            # 标准端点：使用兼容层
            client = DeepSeekCompatibilityClient(
                api_key=config.api_key,
                base_url=config.base_url
            )
            RETURN client

第二步：创建Embedding客户端（硅基流动）

SiliconFlowEmbeddingClient类：
    职责：创建硅基流动Embedding客户端
    
    实现逻辑：
        使用OpenAIGenericClient，因为它能自动处理Schema
        
    配置：
        client = OpenAIGenericClient(
            api_key=config.siliconflow_api_key,
            base_url=config.siliconflow_base_url,
            model=config.siliconflow_embedding_model
        )
        
    支持的模型：
        - BAAI/bge-m3（推荐，多语言支持）
        - BAAI/bge-large-zh-v1.5（中文优化）
        - BAAI/bge-small-zh-v1.5（快速但精度较低）

第三步：创建Reranker客户端（硅基流动）

SiliconFlowRerankerClient类：
    职责：创建硅基流动Reranker客户端
    
    实现逻辑：
        Reranker API不是标准的OpenAI兼容接口
        需要自定义客户端封装
        
    API调用示例：
        POST https://api.siliconflow.cn/v1/rerank
        
        请求体：
        {
            "model": "BAAI/bge-reranker-v2-m3",
            "query": "用户的查询",
            "documents": [
                "文档1的内容",
                "文档2的内容"
            ],
            "top_n": 5
        }
        
        响应体：
        {
            "results": [
                {
                    "index": 2,
                    "document": "文档3的内容",
                    "relevance_score": 0.95
                },
                {
                    "index": 0,
                    "document": "文档1的内容",
                    "relevance_score": 0.87
                }
            ]
        }

第四步：初始化Graphiti实例

GraphitiManager类：
    职责：管理Graphiti实例的生命周期
    
    初始化流程：
        1. 加载配置
           从环境变量和配置文件加载所有参数
        
        2. 创建Neo4j驱动
           driver = GraphDatabase.driver(
               uri="bolt://neo4j:7687",
               auth=(user, password)
           )
        
        3. 测试数据库连接
           driver.verify_connectivity()
        
        4. 创建LLM客户端（DeepSeek）
           llm_client = DeepSeekLLMClientFactory.create_deepseek_client(
               api_key=os.getenv("DEEPSEEK_API_KEY"),
               base_url=os.getenv("DEEPSEEK_BASE_URL")
           )
        
        5. 创建Embedding客户端（硅基流动）
           embedding_client = OpenAIGenericClient(
               api_key=os.getenv("SILICONFLOW_API_KEY"),
               base_url=os.getenv("SILICONFLOW_BASE_URL"),
               model=os.getenv("SILICONFLOW_EMBEDDING_MODEL")
           )
        
        6. 创建Reranker客户端（硅基流动）
           reranker_client = SiliconFlowRerankerClient(
               api_key=os.getenv("SILICONFLOW_API_KEY"),
               base_url=os.getenv("SILICONFLOW_BASE_URL"),
               model=os.getenv("SILICONFLOW_RERANKER_MODEL")
           )
        
        7. 初始化Graphiti实例
           graphiti = Graphiti(
               driver=driver,
               llm_client=llm_client,
               embedding_client=embedding_client,
               cross_encoder_client=reranker_client
           )
        
        8. 初始化数据库结构
           await graphiti.initialize()
           # 创建索引和约束
        
        9. 运行健康检查
           await graphiti.health_check()
```

#### 硅基流动API配置详解

##### Embedding API配置

```python
# 使用OpenAIGenericClient（Graphiti推荐方式）

from openai import OpenAI
import os

# 创建Embedding客户端
embedding_client = OpenAI(
    api_key=os.getenv("SILICONFLOW_API_KEY"),
    base_url=os.getenv("SILICONFLOW_BASE_URL"),  # https://api.siliconflow.cn/v1
)

# 调用Embedding API
response = embedding_client.embeddings.create(
    model=os.getenv("SILICONFLOW_EMBEDDING_MODEL"),  # BAAI/bge-m3
    input="要嵌入的文本"
)

embedding_vector = response.data[0].embedding

# Graphiti会自动使用这个客户端进行向量化
```

##### Reranker API配置

```python
# 硅基流动的Reranker API不是标准的OpenAI兼容接口
# 需要自定义客户端封装

import httpx
import os

class SiliconFlowRerankerClient:
    def __init__(self, api_key, base_url, model):
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.client = httpx.AsyncClient(
            headers={"Authorization": f"Bearer {api_key}"}
        )
    
    async def rerank(self, query, documents, top_n=5):
        """
        对文档进行重排序
        
        参数：
            query: 用户查询
            documents: 文档列表
            top_n: 返回的顶部文档数量
        
        返回：
            按相关性排序的文档列表，带相关性分数
        """
        url = f"{self.base_url}/rerank"
        
        payload = {
            "model": self.model,
            "query": query,
            "documents": documents,
            "top_n": top_n
        }
        
        response = await self.client.post(url, json=payload)
        response.raise_for_status()
        
        result = response.json()
        return result["results"]
    
    async def close(self):
        await self.client.aclose()

# Graphiti会使用这个客户端进行搜索结果重排序
```

#### Graphiti配置的环境变量映射

```python
import os
from dataclasses import dataclass

@dataclass
class GraphitiConfig:
    # Neo4j配置
    neo4j_uri: str
    neo4j_user: str
    neo4j_password: str
    
    # DeepSeek LLM配置
    deepseek_api_key: str
    deepseek_base_url: str
    deepseek_model: str
    
    # 硅基流动Embedding配置
    siliconflow_api_key: str
    siliconflow_base_url: str
    siliconflow_embedding_model: str
    
    # 硅基流动Reranker配置
    siliconflow_reranker_model: str
    
    # Graphiti特定配置
    semaphore_limit: int = 5
    telemetry_enabled: bool = False
    
    @classmethod
    def from_env(cls):
        """从环境变量加载配置"""
        return cls(
            # Neo4j
            neo4j_uri=os.getenv("NEO4J_URI", "bolt://localhost:7687"),
            neo4j_user=os.getenv("NEO4J_USER", "neo4j"),
            neo4j_password=os.getenv("NEO4J_PASSWORD"),
            
            # DeepSeek
            deepseek_api_key=os.getenv("DEEPSEEK_API_KEY"),
            deepseek_base_url=os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com/beta"),
            deepseek_model=os.getenv("DEEPSEEK_MODEL", "deepseek-chat"),
            
            # SiliconFlow
            siliconflow_api_key=os.getenv("SILICONFLOW_API_KEY"),
            siliconflow_base_url=os.getenv("SILICONFLOW_BASE_URL", "https://api.siliconflow.cn/v1"),
            siliconflow_embedding_model=os.getenv("SILICONFLOW_EMBEDDING_MODEL", "BAAI/bge-m3"),
            siliconflow_reranker_model=os.getenv("SILICONFLOW_RERANKER_MODEL", "BAAI/bge-reranker-v2-m3"),
            
            # Graphiti
            semaphore_limit=int(os.getenv("GRAPHITI_SEMAPHORE_LIMIT", "5")),
            telemetry_enabled=os.getenv("GRAPHITI_TELEMETRY_ENABLED", "false").lower() == "true"
        )
    
    def validate(self):
        """验证配置有效性"""
        if not self.deepseek_api_key:
            raise ValueError("DEEPSEEK_API_KEY is required")
        
        if not self.siliconflow_api_key:
            raise ValueError("SILICONFLOW_API_KEY is required")
        
        if not self.neo4j_password:
            raise ValueError("NEO4J_PASSWORD is required")
        
        # 验证DeepSeek端点格式
        valid_endpoints = [
            "https://api.deepseek.com",
            "https://api.deepseek.com/beta"
        ]
        if self.deepseek_base_url not in valid_endpoints:
            raise ValueError(f"Invalid DEEPSEEK_BASE_URL: {self.deepseek_base_url}")
        
        # 验证硅基流动端点格式
        if not self.siliconflow_base_url.startswith("https://api.siliconflow.cn"):
            raise ValueError(f"Invalid SILICONFLOW_BASE_URL: {self.siliconflow_base_url}")
```

#### 完整的初始化示例

```python
import asyncio
from graphiti_core import Graphiti
from neo4j import GraphDatabase

async def initialize_graphiti():
    # 1. 加载配置
    config = GraphitiConfig.from_env()
    config.validate()
    
    # 2. 创建Neo4j驱动
    driver = GraphDatabase.driver(
        uri=config.neo4j_uri,
        auth=(config.neo4j_user, config.neo4j_password)
    )
    
    # 3. 创建DeepSeek LLM客户端
    llm_client = DeepSeekLLMClientFactory.create_deepseek_client(
        api_key=config.deepseek_api_key,
        base_url=config.deepseek_base_url
    )
    
    # 4. 创建硅基流动Embedding客户端
    embedding_client = OpenAI(
        api_key=config.siliconflow_api_key,
        base_url=config.siliconflow_base_url
    )
    
    # 5. 创建硅基流动Reranker客户端
    reranker_client = SiliconFlowRerankerClient(
        api_key=config.siliconflow_api_key,
        base_url=config.siliconflow_base_url,
        model=config.siliconflow_reranker_model
    )
    
    # 6. 初始化Graphiti
    graphiti = Graphiti(
        driver=driver,
        llm_client=llm_client,
        embedding_client=embedding_client,
        cross_encoder_client=reranker_client,
        semaphore_limit=config.semaphore_limit
    )
    
    # 7. 初始化数据库结构
    await graphiti.initialize()
    print("Graphiti initialized successfully")
    
    return graphiti, driver

# 使用示例
async def main():
    graphiti, driver = await initialize_graphiti()
    
    try:
        # 添加Episode
        await graphiti.add_episode(
            episode_id="test-episode",
            name="测试对话",
            content="用户说：你好。AI回复：你好！有什么可以帮助你的吗？"
        )
        
        # 搜索记忆
        results = await graphiti.search(
            query="用户刚才说了什么",
            limit=5
        )
        print(f"搜索结果: {results}")
    
    finally:
        await driver.close()

if __name__ == "__main__":
    asyncio.run(main())
```

#### 关键注意事项

1. **OpenAIGenericClient的使用**
   - 硅基流动的Embedding API是OpenAI兼容的
   - 直接使用OpenAI客户端，base_url指向硅基流动
   - Graphiti会自动处理向量化和存储

2. **Reranker的自定义封装**
   - 硅基流动的Reranker API不是标准的OpenAI兼容接口
   - 需要使用httpx或aiohttp直接调用
   - 实现rerank()方法，接受query和documents参数

3. **并发限制**
   - GRAPHITI_SEMAPHORE_LIMIT控制并发API调用
   - 避免触发API限流（429错误）
   - 根据API提供商的限制调整此值

4. **模型选择**
   - BAAI/bge-m3: 平衡性能和速度（推荐）
   - BAAI/bge-large-zh-v1.5: 中文优化，精度更高
   - BAAI/bge-reranker-v2-m3: 推荐的Reranker模型

5. **错误处理**
   - 实现重试机制
   - 处理API限流
   - 记录详细的错误日志

---

## 5. API服务实现

### 5.1 API服务架构

#### 整体架构设计

```
API服务分层架构：

┌─────────────────────────────────────────┐
│         FastAPI应用层               │
│  • 路由定义                         │
│  • 请求验证                         │
│  • 响应格式化                       │
└────────────────┬────────────────────────┘
                 │
    ┌────────────┼────────────┐
    │            │            │
    ▼            ▼            ▼
┌────────┐  ┌────────┐  ┌────────┐
│ 格式   │  │ 记忆   │  │ LLM    │
│ 解析器 │  │ 管理器 │  │ 调用器 │
└────────┘  └────────┘  └────────┘
    │            │            │
    └────────────┼────────────┘
                 │
                 ▼
         ┌────────────────┐
         │  Graphiti核心   │
         └────────────────┘
                 │
                 ▼
         ┌────────────────┐
         │   Neo4j数据库   │
         └────────────────┘
```

### 5.2 OpenAI兼容API端点设计

#### 端点：/v1/chat/completions

**请求处理逻辑**：

```
POST /v1/chat/completions 处理流程：

1. 请求接收和验证
    - 验证请求格式（符合OpenAI规范）
    - 提取session_id（自定义header或请求体）
    - 验证API密钥（可选，用于限流）

2. SillyTavern内容提取
    - 从messages数组中提取最后一个user消息
    - 这通常是包含完整上下文的提示词
    - 提取内容包括：
        * 核心指导（<核心指导>...</核心指导>）
        * 世界书（<相关资料>...</相关资料>）
        * 角色描述
        * 对话历史（<互动历史>...</互动历史>）
        * 补充资料

3. 格式解析
    - 使用正则表达式识别标签
    - 应用启发式规则分类内容
    - 构建结构化内容对象：
        ParsedContent:
            - instructions: List[str]  # 指令性内容
            - narratives: List[NarrativeBlock]  # 叙事性内容
            - chat_history: List[DialogTurn]  # 对话历史
            - metadata: Dict[str, Any]  # 元数据

4. 记忆处理（异步）
    - 对于叙事性内容，调用Graphiti处理
    - 流程：
        a. 实体关系提取
        b. 去重和合并
        c. 存储到知识图谱
        d. 检索相关记忆
    
    - 指令性内容直接保留，不入图谱

5. 上下文优化
    - 构建增强的提示词：
        * 保留所有指令性内容（完整）
        * 用Graphiti召回的相关记忆替换部分叙事性内容
        * 保留最近N轮对话历史
        * 优化Token使用
    
    - Token优化策略：
        IF 预估Token > 8000:
            - 摘要远期记忆
            - 移除低相关性内容
            - 保留最新对话

6. LLM调用
    - 调用DeepSeek API
    - 传递优化后的消息
    - 接收响应

7. 响应后处理
    - 从LLM响应中提取新信息
    - 存储到Graphiti（异步，不阻塞响应）
    - 格式化为OpenAI兼容响应

8. 返回响应
    - 标准OpenAI Chat Completions格式
    - 包含自定义header：
        * X-Session-ID: 会话ID
        * X-Memory-Entities: 存储的实体数
        * X-Related-Memories: 检索的记忆数
```

#### 响应格式

```python
# OpenAI兼容响应结构
{
    "id": "chatcmpl-{uuid}",           # 唯一请求ID
    "object": "chat.completion",          # 对象类型
    "created": 17034567890,            # Unix时间戳
    "model": "deepseek-chat",          # 使用的模型
    "choices": [{
        "index": 0,
        "message": {
            "role": "assistant",
            "content": "AI生成的回复内容..."
        },
        "finish_reason": "stop"
    }],
    "usage": {
        "prompt_tokens": 1500,
        "completion_tokens": 800,
        "total_tokens": 2300
    }
}
```

### 5.3 SillyTavern格式解析器设计

#### 解析策略

```
SillyTavern格式解析器设计：

Parser类：
    职责：解析SillyTavern的特殊格式输入
    
    主要方法：
        parse(input_text: str) → ParsedContent
    
    解析流程：
        1. 多级标签检测
           - 第一级：精确标签匹配（<核心指导>等）
           - 第二级：模糊标签识别（启发式）
           - 第三级：模式匹配（User/Assistant交替）
        
        2. 内容提取
           - 提取标签内容
           - 识别内容类型
           - 构建内容块对象
        
        3. 结构化表示
           - 转换为统一的数据结构
           - 保留位置信息
           - 记录提取置信度

标签检测逻辑：

检测规则集：
    规则1: 精确标签匹配
        IF 正则匹配<([^/>]+)>([\s\S]*?)</\1>:
            识别为结构化标签
            提取标签名和内容
    
    规则2: 特殊格式标签
        IF 匹配<\|([^|]+)\|>:
            识别为竖线分隔标签
            常见：<|User|>, <|Assistant|>
    
    规则3: 对话历史模式
        IF 检测到User:和Assistant:交替:
            识别为对话历史
            提取说话者和内容
    
    规则4: 世界书条目
        IF 匹配地点\([^)]+\)或角色\([^)]+\):
            识别为世界书条目
            解析属性列表

内容分类逻辑：

分类决策树：
    开始分类
        │
        ├─ 有明确标签？
        │   └─ 是 → 按标签映射表分类
        │       ├── <核心指导> → instruction
        │       ├── <基础风格> → instruction
        │       ├── <相关资料> → narrative (world_info)
        │       └── <互动历史> → narrative (dialog)
        │
        └─ 否 → 应用启发式规则
            ├── 包含"User:"/"Assistant:"交替 → dialog_history
            ├── 包含"地点("/"角色(" → world_info
            ├── 包含"必须"/"不得" → instruction
            └── 默认 → general_content

并行处理设计：

对于大型内容（如世界书）：
    1. 分条目分割
       - 基于分隔符（空行、标题）
       - 生成独立条目列表
    
    2. 任务队列分发
       - 创建固定大小的工作线程池
       - 将条目均匀分配到队列
    
    3. 并行处理
       - 每个线程独立处理分配的条目
       - 应用相同的处理逻辑
       - 互不影响
    
    4. 结果收集
       - 等待所有线程完成
       - 合并处理结果
       - 去重和冲突解决

伪代码表示：

并行处理流程：
    
    def process_world_info_parallel(entries, max_workers=4):
        # 创建任务队列
        task_queue = Queue()
        result_queue = Queue()
        
        # 分发任务
        for entry in entries:
            task_queue.put(entry)
        
        # 创建工作线程
        workers = []
        for i in range(max_workers):
            worker = ProcessingWorker(
                task_queue, 
                result_queue, 
                worker_id=i
            )
            worker.start()
            workers.append(worker)
        
        # 添加终止信号
        for _ in range(max_workers):
            task_queue.put(None)
        
        # 等待完成
        for worker in workers:
            worker.join()
        
        # 收集结果
        results = []
        while not result_queue.empty():
            results.append(result_queue.get())
        
        return merge_results(results)
```

---

## 6. DeepSeek集成方案

### 6.1 集成策略总结

基于搜索结果，我们提供**两种集成方案**：

#### 方案A：使用Beta端点 + Strict模式（推荐）

**适用场景**: 生产环境，追求最佳质量和稳定性

**优势**:
- DeepSeek完全支持JSON Schema
- 无需额外兼容层
- 服务器端验证Schema有效性
- 输出质量更高

**实施步骤**:
1. 修改.env，设置`DEEPSEEK_BASE_URL=https://api.deepseek.com/beta`
2. 在所有Function Call工具定义中添加`strict: true`
3. 确保JSON Schema符合DeepSeek要求
4. Graphiti直接使用Strict模式客户端

**Schema要求**:
```python
{
    "type": "object",
    "properties": {
        "field_name": {
            "type": "string",  # 或number, integer, boolean, array
            "description": "字段描述"
        }
    },
    "required": ["field_name"],  # 所有字段必须required
    "additionalProperties": false  # 必须为false
}
```

#### 方案B：标准端点 + 兼容层

**适用场景**: Beta端点不可用或需要快速部署

**优势**:
- 使用标准端点，更稳定
- 兼容层处理所有转换逻辑
- 降级方案，可随时切换回方案A

**劣势**:
- 需要额外的验证和转换步骤
- 可能增加处理延迟
- 准确度略低于Strict模式

**实施步骤**:
1. 使用标准端点`https://api.deepseek.com`
2. 实现兼容层Client类（上文已详细描述）
3. 在调用前增强提示词
4. 在接收后验证和修复JSON
5. 提供重试机制

### 6.2 完整的LLM服务层设计

#### LLM服务架构

```
LLM服务层组件：

LLMService类：
    职责：封装所有LLM调用逻辑
    
    主要方法：
        - async generate_completion()
        - async call_with_tools()
        - async test_connection()
    
    配置：
        - LLM提供商选择（DeepSeek）
        - 模型配置
        - API密钥和端点
        - 重试策略
        - 超时设置

DeepSeekLLMClient类：
    继承自：BaseLLMClient
    
    特殊处理：
        - 根据端点类型选择客户端
        - 实现Strict模式（Beta端点）
        - 实现兼容层（标准端点）
        - 错误处理和重试

调用流程：

标准调用（无工具）：
    1. 构建消息列表
    2. 调用chat.completions.create()
    3. 提取content字段
    4. 返回结果

工具调用（Function Calling）：
    1. 构建messages + tools参数
    2. 根据端点类型：
       IF Beta端点:
           - 在tools中添加strict: true
           - 调用API
           - 直接获取tool_calls
       ELSE 标准端点:
           - 使用兼容层
           - 增强提示词
           - 验证输出
    3. 提取工具调用结果
    4. 返回结构化结果

错误处理策略：
    
    错误分类：
        - API错误（4xx, 5xx）
        - 验证错误（JSON格式错误）
        - 超时错误
        - 配额错误（429）
    
    重试策略：
        - API错误：重试3次，指数退避
        - 验证错误：修复后重试1次
        - 超时错误：增加超时，重试1次
        - 配额错误：等待后重试
    
    降级策略：
        - 如果Strict模式失败：切换到兼容层
        - 如果兼容层失败：简化Schema重试
        - 如果仍失败：返回错误提示
```

---

## 7. SillyTavern连接配置

### 7.1 SillyTavern端配置步骤

#### 连接配置界面

1. **打开SillyTavern设置**
   - 点击右上角设置图标
   - 进入"连接"选项卡

2. **配置API端点**
   ```
   后端提供商：OpenAI（或自定义）
   API URL：http://your-server-ip:8000/v1/chat/completions
   API Key：任意值（我们的API不验证，但SillyTavern需要）
   ```

3. **模型配置**
   ```
   模型名称：deepseek-chat
   （或deepseek-v3.2，根据API返回的model字段）
   ```

4. **上下文设置**
   ```
   上下文长度：8192（推荐）
   启用世界信息：是
   启用角色定义：是
   启用作者备注：否
   ```

### 7.2 会话管理

#### 会话ID机制

```
会话ID管理策略：

ID生成：
    - 首次请求：自动生成UUID
    - 后续请求：从请求中读取
    - 格式：sess-{uuid}

传递方式：
    方式1：自定义HTTP Header
        - Header名称：X-Session-ID
        - 优点：不影响请求体格式
        - 缺点：SillyTavern可能不发送自定义header
    
    方式2：请求体嵌入（推荐）
        - 在messages中插入系统消息：
          {
            "role": "system",
            "content": "SESSION_ID: sess-12345678"
          }
        - 优点：兼容性最好
        - 缺点：占用token
    
    方式3：请求参数
        - 添加自定义字段：
          {
            "model": "deepseek-chat",
            "messages": [...],
            "session_id": "sess-12345678"
          }
        - 优点：不污染内容
        - 缺点：不符合OpenAI规范

推荐实现：
    组合使用方式1和方式3
    - 优先检查自定义header
    - 其次检查请求参数
    - 最后解析系统消息内容
```

### 7.3 格式兼容性测试

#### 测试用例

```python
# 测试脚本思路

测试用例1：基础对话
请求：
    {
        "model": "deepseek-chat",
        "messages": [
            {"role": "user", "content": "你好"}
        ]
    }
预期：正常响应

测试用例2：带核心指导
请求：
    {
        "model": "deepseek-chat",
        "messages": [
            {
                "role": "user",
                "content": """
                <核心指导>
                你是一个中文创作助手。
                </核心指导>
                Hello
                """
            }
        ]
    }
预期：核心指导被保留，不入图谱

测试用例3：带世界书
请求：
    {
        "model": "deepseek-chat",
        "messages": [
            {
                "role": "user",
                "content": """
                <相关资料>
                地点("基沃托斯")["学园城市"]
                角色("圣园未花")["16岁少女"]
                </相关资料>
                Hi
                """
            }
        ]
    }
预期：世界书被Graphiti处理，相关记忆被检索

测试用例4：带对话历史
请求：
    {
        "model": "deepseek-chat",
        "messages": [
            {
                "role": "user",
                "content": """
                <互动历史>
                User: 你好
                Assistant: 你好！有什么可以帮助你的吗？
                User: 请介绍一下圣园未花
                </互动历史>
                """
            }
        ]
    }
预期：对话历史被处理，记忆被存储
```

---

## 8. 测试与验证

### 8.1 部署后验证步骤

#### 完整验证流程

```
验证步骤1：基础设施检查
    ├─ 检查Docker容器状态
    │   命令：docker-compose ps
    │   期望：所有容器状态为Up
    │
    ├─ 检查Neo4j连接
    │   命令：curl http://localhost:7474
    │   或：浏览器访问http://localhost:7474
    │   期望：能登录Neo4j Browser
    │
    ├─ 检查API服务健康
    │   命令：curl http://localhost:8000/health
    │   期望：返回{"status": "healthy"}
    │
    └─ 检查日志
        命令：docker-compose logs
        期望：无ERROR级别的日志

验证步骤2：Graphiti功能测试
    ├─ 测试Neo4j连接
    │   在Neo4j Browser中运行：
    │   RETURN 1
    │   期望：返回1
    │
    ├─ 测试Graphiti初始化
    │   在Python中：
    │   from graphiti_core import Graphiti
    │   graphiti = Graphiti("bolt://localhost:7687", "neo4j", "password")
    │   期望：无错误
    │
    └─ 测试简单Episode添加
        添加测试数据，查询验证

验证步骤3：API端点测试
    ├─ 测试健康检查
    │   curl http://localhost:8000/health
    │
    ├─ 测试OpenAI兼容端点
    │   发送简单对话请求
    │   检查响应格式
    │   检查自定义header
    │
    └─ 测试记忆功能
        发送带世界书的请求
        查询记忆端点验证数据

验证步骤4：SillyTavern集成测试
    ├─ 配置SillyTavern连接
    │
    ├─ 测试基础对话
    │   发送简单消息
    │   检查是否能正常回复
    │
    ├─ 测试带世界书的角色卡
    │   使用完整角色卡测试
    │   检查记忆是否生效
    │
    └─ 测试多轮对话
        进行多轮交互
        检查记忆一致性
```

### 8.2 性能基准测试

#### 测试指标

```
性能测试项目：

1. 响应时间
    - 目标：P95 < 5秒
    - 测试：发送100个请求，统计延迟分布
    - 优化：超过目标时分析瓶颈

2. 吞吐量
    - 目标：100 请求/分钟
    - 测试：持续负载测试
    - 优化：调整Worker数量

3. 内存使用
    - 目标：Neo4j < 8GB, API < 4GB
    - 监控：docker stats
    - 优化：限制并发，优化查询

4. Token使用
    - 目标：上下文优化后节省30%
    - 测试：对比优化前后token数
    - 优化：调整摘要策略

5. Graphiti存储效率
    - 目标：每个Episode < 100个节点
    - 测试：添加1000个Episode，统计节点数
    - 优化：调整去重阈值
```

---

## 9. 故障排除

### 9.1 常见问题及解决方案

#### 问题1：Neo4j启动失败

**症状**:
```
docker-compose up neo4j
neo4j容器启动后立即退出
日志显示：java.lang.OutOfMemoryError
```

**解决方案**:
```bash
# 降低内存配置
# 编辑.env
NEO4J_dbms_memory_pagecache_size=1G  # 从2G降低
NEO4J_dbms_memory_heap_initial__size=2G  # 从4G降低
NEO4J_dbms_memory_heap_max__size=2G  # 从4G降低

# 重启服务
docker-compose restart neo4j
```

#### 问题2：API服务无法连接Neo4j

**症状**:
```
API日志显示：
neo4j.exceptions.ServiceUnavailable: Unable to connect to bolt://neo4j:7687
```

**解决方案**:
```bash
# 检查Neo4j容器状态
docker-compose ps neo4j

# 检查网络连接
docker-compose exec api-service ping neo4j

# 检查端口开放
docker-compose exec api-service nc -zv neo4j 7687

# 如果连接失败，重启Neo4j
docker-compose restart neo4j
```

#### 问题3：DeepSeek API调用失败

**症状A**：Strict模式错误
```
错误信息：4xx Bad Request
错误详情：Invalid JSON Schema
```
**解决方案**:
```bash
# 方案1：检查Schema格式
确保所有object属性满足：
- type: "object"
- required字段包含所有属性
- additionalProperties: false

# 方案2：降级到兼容层
修改.env：
DEEPSEEK_BASE_URL=https://api.deepseek.com  # 不使用/beta
```

**症状B**：429 Rate Limit
```
错误信息：429 Too Many Requests
```
**解决方案**:
```bash
# 降低并发
修改.env：
GRAPHITI_SEMAPHORE_LIMIT=3  # 从5降低

# 增加重试延迟
在代码中实现指数退避：
retry 1: wait 1s
retry 2: wait 2s
retry 3: wait 4s
```

**症状C**：认证失败
```
错误信息：401 Unauthorized
```
**解决方案**:
```bash
# 检查API Key
echo $DEEPSEEK_API_KEY

# 确认API Key有效
curl -X POST "https://api.deepseek.com/v1/chat/completions" \
  -H "Authorization: Bearer $DEEPSEEK_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"model":"deepseek-chat","messages":[{"role":"user","content":"Hi"}]}'
```

#### 问题4：SillyTavern无法连接

**症状**:
```
SillyTavern显示：连接失败
错误信息：Connection refused
```
**解决方案**:
```bash
# 检查API服务是否运行
docker-compose ps api-service

# 检查端口是否开放
netstat -tuln | grep 8000
# 或
ss -tuln | grep 8000

# 检查防火墙
# Ubuntu:
sudo ufw allow 8000
# CentOS:
sudo firewall-cmd --add-port=8000/tcp --permanent
sudo firewall-cmd --reload

# 检查API健康
curl http://localhost:8000/health
curl http://your-server-ip:8000/health
```

#### 问题5：记忆功能不生效

**症状**:
```
SillyTavern正常对话，但记忆查询为空
Neo4j Browser中看不到数据
```

**诊断步骤**:
```bash
# 1. 检查API日志
docker-compose logs api-service --tail 100

# 2. 查找Graphiti相关日志
# 查找是否正确调用了Graphiti
# 查找是否有错误信息

# 3. 测试Graphiti存储
docker-compose exec api-service python3 << 'EOF'
import asyncio
from graphiti_core import Graphiti

async def test():
    g = Graphiti("bolt://neo4j:7687", "neo4j", "password")
    await g.initialize()
    # 测试添加Episode
    print("Graphiti test completed")

asyncio.run(test())
EOF

# 4. 检查Neo4j数据
# 在Neo4j Browser中运行：
MATCH (n) RETURN count(n) as node_count
# 应该返回 > 0
```

**解决方案**:
```
可能原因1：LLM返回格式错误
→ 检查兼容层日志，修复Schema验证

可能原因2：去重导致数据被过滤
→ 降低去重阈值，检查相似度计算

可能原因3：解析器无法识别格式
→ 查看解析日志，调整正则表达式

可能原因4：权限问题
→ 检查Neo4j用户权限
→ 检查文件系统权限
```

### 9.2 调试模式

#### 启用详细日志

```bash
# 1. 修改.env
APP_DEBUG=true
API_LOG_LEVEL=debug

# 2. 重启服务
docker-compose restart api-service

# 3. 查看详细日志
docker-compose logs -f api-service

# 4. 临时启用特定模块日志
docker-compose exec api-service python3 << 'EOF'
import logging
logging.getLogger('graphiti_core').setLevel(logging.DEBUG)
logging.getLogger('services.llm_service').setLevel(logging.DEBUG)
EOF
```

#### Neo4j查询调试

```cypher
-- 在Neo4j Browser中运行

-- 查看最近添加的节点
MATCH (n)
WHERE n.created_at > datetime() - duration('P1D')
RETURN n.type, count(n)
ORDER BY count(n) DESC

-- 查看最近的Episode
MATCH (e:Episode)
RETURN e.episode_id, e.created_at, e.name
ORDER BY e.created_at DESC
LIMIT 10

-- 查看关系数量
MATCH ()-[r]->()
RETURN type(r), count(r)
ORDER BY count(r) DESC

-- 检查数据完整性
MATCH (n)
WHERE n.uuid IS NULL
RETURN count(n) as missing_uuid_count
-- 应该返回0
```

### 9.3 性能优化建议

#### Neo4j优化

```cypher
-- 创建必要的索引
CREATE INDEX character_name_index IF NOT EXISTS
FOR (c:Character) ON (c.name);

CREATE INDEX entity_timestamp_index IF NOT EXISTS
FOR (e:Entity) ON (e.created_at);

CREATE INDEX episode_session_index IF NOT EXISTS
FOR (ep:Episode) ON (ep.session_id);

-- 全文索引
CREATE FULLTEXT INDEX entity_search_index IF NOT EXISTS
FOR (e:Entity) ON EACH [e.name, e.description];
```

#### API服务优化

```python
# 启用响应缓存
from functools import lru_cache

@lru_cache(maxsize=100)
async def get_memory_summary(session_id):
    # 缓存记忆摘要
    pass

# 启用连接池
import aiohttp

connector = aiohttp.TCPConnector(limit=100, ttl_dns_cache=300)
```

---

## 附录A：快速启动检查清单

### 部署前检查

- [ ] Docker和Docker Compose已安装
- [ ] Docker Compose版本 >= 2.0
- [ ] 端口7474, 7687, 8000未被占用
- [ ] 磁盘空间充足（至少50GB可用）
- [ ] 内存充足（至少8GB可用）
- [ ] DeepSeek API Key有效

### 配置检查

- [ ] .env文件已创建
- [ ] 所有密码和密钥已修改为实际值
- [ ] NEO4J_URI配置正确（bolt://neo4j:7687用于容器内）
- [ ] DEEPSEEK_API_KEY已设置
- [ ] DEEPSEEK_BASE_URL根据需求选择（标准或beta）
- [ ] .env文件权限正确（chmod 600）

### 启动检查

- [ ] docker-compose up -d 成功执行
- [ ] 所有容器状态为Up（docker-compose ps）
- [ ] Neo4j健康检查通过（docker-compose logs neo4j）
- [ ] API服务健康检查通过（curl localhost:8000/health）

### 功能验证检查

- [ ] Neo4j Browser可访问（http://localhost:7474）
- [ ] 可以在Neo4j中查询数据
- [ ] API端点返回健康状态
- [ ] OpenAI兼容端点响应正常
- [ ] SillyTavern可以连接并获取回复
- [ ] 世界书和对话历史被Graphiti处理
- [ ] 记忆检索功能正常工作

---

## 附录B：安全最佳实践

### 生产环境安全配置

```bash
# 1. 限制Neo4j Browser访问
# 仅本地访问
# 修改docker-compose.yaml，移除7474端口映射
# 或使用反向代理+认证

# 2. 启用HTTPS
# 使用Nginx反向代理
# 配置SSL证书
# 强制HTTPS重定向

# 3. 限制API访问
# 使用防火墙规则
# 实施IP白名单
# 添加速率限制

# 4. 加密敏感数据
# .env文件加密
# 使用密钥管理服务
# 定期轮换密钥

# 5. 监控和审计
# 启用访问日志
# 监控异常行为
# 定期安全审计
```

---

## 附录C：监控和告警

### 关键指标

```
健康指标：
- API响应时间（P50, P95, P99）
- 错误率（4xx, 5xx）
- 请求吞吐量（RPS）
- Neo4j查询时间
- DeepSeek API延迟

资源指标：
- CPU使用率
- 内存使用率
- 磁盘I/O
- 网络带宽

业务指标：
- 活跃会话数
- 存储的Episode数量
- 检索请求次数
- Token使用量
```

### 告警规则

```
告警级别定义：

警告（WARNING）:
- API响应时间 > 3秒
- 错误率 > 1%
- CPU使用率 > 70%
- 内存使用率 > 80%

严重（CRITICAL）:
- API响应时间 > 10秒
- 错误率 > 5%
- 服务不可用
- 磁盘使用率 > 90%

告警通知方式：
- 邮件通知
- Slack/Teams集成
- 短信通知（关键告警）
```

---

## 总结

本指南提供了完整的AIRP记忆系统开发部署方案，从环境准备到生产运维。关键要点：

1. **技术选型**：Graphiti + Neo4j + DeepSeek V3.2 + FastAPI
2. **兼容层设计**：两种方案解决JSON Schema兼容问题
3. **Docker部署**：完整的docker-compose配置和部署流程
4. **SillyTavern集成**：OpenAI兼容API，无需修改SillyTavern
5. **测试验证**：完整的测试流程和性能基准
6. **故障排除**：常见问题的诊断和解决方案

建议实施顺序：
1. 先在开发环境完成搭建和测试
2. 使用方案A（Beta端点）进行初步集成
3. 根据实际效果决定是否切换到方案B
4. 完成功能测试后，进行性能优化
5. 部署到生产环境前，进行完整的安全审计

祝您的项目顺利实施！
